{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b80c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kaifzaki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kaifzaki/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kaifzaki/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "# Natural Language Toolkit for text processing\n",
    "import nltk\n",
    "# SSL for secure connections\n",
    "import ssl\n",
    "# Regular expressions for text cleaning\n",
    "import re\n",
    "# Numerical operations\n",
    "import numpy as np\n",
    "#stopwords for removing common words\n",
    "from nltk.corpus import stopwords\n",
    "#lemmatizer use for text normalization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#TF-IDF vectorizer for converting text to numerical data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#KMeans for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "#Silhouette score for evaluating clustering quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "try:\n",
    " _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    " ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60373fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wikipediaapi\n",
    "\n",
    "# wiki_api = wikipediaapi.Wikipedia('MyClustteringProject','en')\n",
    "# page = wiki_api.page(\"machine learning\")\n",
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d29c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched article: Galax\n",
      "Fetched article: Black hole\n",
      "Fetched article: supernova\n",
      "Fetched article: DNA\n",
      "Fetched article: Photosynthesis\n",
      "Fetched article: Evolution\n",
      "Fetched article: machine learning\n",
      "Fetched article: Artificial intelligence\n",
      "Fetched article: computer programming\n",
      "Total articles fetched: 9\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "artical_titles = [\"Galax\",\"Black hole\",\"supernova\",\n",
    "                  \"DNA\",\"Photosynthesis\",\"Evolution\",\n",
    "                  \"machine learning\",\"Artificial intelligence\",\"computer programming\"]\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClustteringProject/1.0','en')\n",
    "\n",
    "documents = []\n",
    "for title in artical_titles:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        print(f\"Fetched article: {title}\")\n",
    "    else:\n",
    "        print(f\"Article not found: {title}\")\n",
    "print(f\"Total articles fetched: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cc6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words :{'at', \"i'll\", 'doesn', \"she's\", 'both', 'we', 'again', 'shouldn', \"they've\", 'don', 'more', 'you', 'the', 'no', 'haven', 'on', 'do', 'such', 'should', \"wasn't\", 'until', \"we'd\", \"they'd\", \"weren't\", 'most', 'been', 'her', 'after', 'is', 'yourselves', 'as', 'below', \"didn't\", \"i'd\", 'into', \"aren't\", 'who', 'if', 'before', 'not', 'for', \"he'd\", 'needn', 'when', \"shan't\", 'themselves', 'y', 'than', 'itself', 'm', \"doesn't\", 'very', 'during', 'isn', \"haven't\", 'having', \"mustn't\", 'have', \"we'll\", 'this', 'because', 'few', 'their', 'how', 'in', 'an', 'was', 'only', 'some', 'hasn', 'of', 'out', 'they', 'ourselves', 'had', \"she'll\", 're', 'did', 'he', \"that'll\", 'up', 'hadn', 'whom', 'wasn', \"don't\", \"you're\", 'above', 'being', 'will', 'be', 'it', \"shouldn't\", 'from', 'just', 'o', 'where', 'what', 'same', 'hers', \"you've\", 'here', 'between', \"wouldn't\", 'down', 'which', 'there', \"they'll\", \"you'd\", 'couldn', \"mightn't\", 'why', 'myself', 'too', 'through', 'nor', 'll', 'aren', \"should've\", \"she'd\", 'our', \"won't\", 'mustn', 'to', 'me', \"isn't\", 'a', 'own', 'but', 'that', 'any', 'your', 'doing', 'once', \"needn't\", 've', 't', 'now', 'further', 'theirs', 'didn', \"hasn't\", 'each', 'him', 'yours', 'my', 'were', 'against', 'i', \"i've\", \"it'd\", 'so', 'his', 'all', 'these', 'wouldn', 's', \"he's\", \"i'm\", \"it's\", 'd', 'weren', 'by', 'shan', 'under', 'ain', 'about', \"he'll\", 'ours', \"we've\", 'has', \"couldn't\", 'does', \"we're\", 'its', 'ma', \"hadn't\", 'over', 'am', \"they're\", 'or', 'off', 'himself', 'yourself', 'and', 'while', \"it'll\", 'with', \"you'll\", 'mightn', 'other', 'are', 'them', 'then', 'won', 'herself', 'can', 'she', 'those'}\n",
      "Number of stopwords: 198\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Stop Words :{stop_words}\")\n",
    "print(f\"Number of stopwords: {len(stop_words)}\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stopwords and lemmatize\n",
    "    processd_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(processd_words)\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(\"Preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893ea2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4703 stored elements and shape (9, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 368)\t0.8512306812056797\n",
      "  (0, 677)\t0.1792181330104813\n",
      "  (0, 189)\t0.04480453325262033\n",
      "  (0, 836)\t0.03627313109833374\n",
      "  (0, 857)\t0.022402266626310164\n",
      "  (0, 953)\t0.02510569981063823\n",
      "  (0, 869)\t0.020116546437431444\n",
      "  (0, 604)\t0.0444834922189141\n",
      "  (0, 398)\t0.04480453325262033\n",
      "  (0, 676)\t0.020116546437431444\n",
      "  (0, 621)\t0.03627313109833374\n",
      "  (0, 582)\t0.01813656554916687\n",
      "  (0, 506)\t0.016390098632453266\n",
      "  (0, 445)\t0.04023309287486289\n",
      "  (0, 547)\t0.014827830739638035\n",
      "  (0, 255)\t0.020116546437431444\n",
      "  (0, 242)\t0.014827830739638035\n",
      "  (0, 948)\t0.02965566147927607\n",
      "  (0, 513)\t0.020116546437431444\n",
      "  (0, 38)\t0.014827830739638035\n",
      "  (0, 983)\t0.016390098632453266\n",
      "  (0, 498)\t0.07531709943191468\n",
      "  (0, 822)\t0.022402266626310164\n",
      "  (0, 710)\t0.01813656554916687\n",
      "  (0, 491)\t0.01813656554916687\n",
      "  :\t:\n",
      "  (8, 167)\t0.007752299703076014\n",
      "  (8, 180)\t0.015504599406152028\n",
      "  (8, 522)\t0.02325689910922804\n",
      "  (8, 500)\t0.007752299703076014\n",
      "  (8, 939)\t0.007752299703076014\n",
      "  (8, 435)\t0.015504599406152028\n",
      "  (8, 185)\t0.007752299703076014\n",
      "  (8, 178)\t0.007752299703076014\n",
      "  (8, 629)\t0.031009198812304056\n",
      "  (8, 432)\t0.03876149851538008\n",
      "  (8, 564)\t0.02325689910922804\n",
      "  (8, 963)\t0.08527529673383616\n",
      "  (8, 389)\t0.007752299703076014\n",
      "  (8, 476)\t0.031009198812304056\n",
      "  (8, 679)\t0.007752299703076014\n",
      "  (8, 404)\t0.05426609792153209\n",
      "  (8, 162)\t0.04651379821845608\n",
      "  (8, 480)\t0.008916109352495481\n",
      "  (8, 870)\t0.008916109352495481\n",
      "  (8, 67)\t0.008916109352495481\n",
      "  (8, 237)\t0.062412765467468365\n",
      "  (8, 716)\t0.2853154992798554\n",
      "  (8, 245)\t0.07132887481996385\n",
      "  (8, 412)\t0.062412765467468365\n",
      "  (8, 169)\t0.10556406925776772\n",
      "TF-IDF matrix created successfully\n",
      "Shape of the matrix: (9, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 : Covert Text to Vectors\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000) # Limit to top 1000 features\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(\"TF-IDF matrix created successfully\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c1aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [1 2 2 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "k =3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42,n_init=5)\n",
    "## the below line is fuctionally identical to the above line\n",
    "# kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"10\"\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get cluster labels for each document\n",
    "labels = kmeans.labels_\n",
    "print(f\"Cluster labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb17299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-Cluster Sum of Squares (WCSS): 4.8690322755338284\n",
      "Silhouette Score: 0.08007362002750984\n"
     ]
    }
   ],
   "source": [
    "wcss = kmeans.inertia_\n",
    "\n",
    "sil_score = silhouette_score(tfidf_matrix, labels)\n",
    "\n",
    "print(f\"Within-Cluster Sum of Squares (WCSS): {wcss}\")\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
