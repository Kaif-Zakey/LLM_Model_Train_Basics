{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b80c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kaifzaki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kaifzaki/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kaifzaki/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "# Natural Language Toolkit for text processing\n",
    "import nltk\n",
    "# SSL for secure connections\n",
    "import ssl\n",
    "# Regular expressions for text cleaning\n",
    "import re\n",
    "# Numerical operations\n",
    "import numpy as np\n",
    "#stopwords for removing common words\n",
    "from nltk.corpus import stopwords\n",
    "#lemmatizer use for text normalization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#TF-IDF vectorizer for converting text to numerical data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#KMeans for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "#Silhouette score for evaluating clustering quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "try:\n",
    " _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    " ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60373fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wikipediaapi\n",
    "\n",
    "# wiki_api = wikipediaapi.Wikipedia('MyClustteringProject','en')\n",
    "# page = wiki_api.page(\"machine learning\")\n",
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d29c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched article: Galax\n",
      "Fetched article: Black hole\n",
      "Fetched article: supernova\n",
      "Fetched article: DNA\n",
      "Fetched article: Photosynthesis\n",
      "Fetched article: Evolution\n",
      "Fetched article: machine learning\n",
      "Fetched article: Artificial intelligence\n",
      "Fetched article: computer programming\n",
      "Total articles fetched: 9\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "artical_titles = [\"Galax\",\"Black hole\",\"supernova\",\n",
    "                  \"DNA\",\"Photosynthesis\",\"Evolution\",\n",
    "                  \"machine learning\",\"Artificial intelligence\",\"computer programming\"]\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClustteringProject/1.0','en')\n",
    "\n",
    "documents = []\n",
    "for title in artical_titles:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        print(f\"Fetched article: {title}\")\n",
    "    else:\n",
    "        print(f\"Article not found: {title}\")\n",
    "print(f\"Total articles fetched: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cc6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words :{'before', 'each', 'is', 'as', 'while', \"wouldn't\", \"they'd\", \"mightn't\", \"i'll\", 'can', 'has', 'into', 'am', \"hadn't\", \"you're\", 'what', \"won't\", \"he'll\", 'a', \"didn't\", 'out', 'too', 'aren', 'y', 'be', \"hasn't\", \"it's\", 'of', 'my', 'until', 'than', 'the', 'needn', 'hasn', \"isn't\", 'does', \"she'll\", 'its', 'both', 't', 'because', 'had', 'doing', 'haven', 'when', 'weren', 'shouldn', 'for', 'now', 'ma', 'by', 'been', 'won', 'you', 'between', 'such', 'some', 'have', 'where', \"they've\", 'themselves', \"i'd\", 'more', 'are', 'himself', 'to', \"haven't\", 'most', 'not', 'your', 'and', 'were', 'do', 'his', \"mustn't\", 'after', 'mustn', 'against', \"shouldn't\", 'during', 'couldn', 'our', 'yourselves', 'no', \"it'll\", 'those', 'did', 'just', 'here', 'doesn', 'o', 'with', \"she's\", 'above', 'so', 'again', 'under', 'shan', 'they', 'wouldn', 'any', 'we', \"it'd\", 'how', 's', 'them', \"they'll\", 'but', 'itself', 'ours', 'from', 'this', \"i'm\", \"wasn't\", \"you've\", \"we've\", 'hers', 'hadn', 'their', 'ain', \"he's\", \"shan't\", \"we're\", \"i've\", \"couldn't\", \"they're\", 'was', 'myself', 'few', 'there', 'wasn', 'having', 'in', 'ourselves', 'being', 'further', 'nor', \"should've\", 'same', \"she'd\", 'didn', 'or', 'own', \"doesn't\", 'her', 'll', 'which', 'd', 'an', 'up', 'if', 'at', 'only', 'why', \"he'd\", \"aren't\", 'down', \"we'd\", 'all', 'theirs', 'below', 'once', 'on', 'yours', \"weren't\", 'that', 'herself', 'yourself', \"you'd\", 'other', 'she', 'me', 'then', \"we'll\", 'should', 'over', 're', 'who', 'he', 'will', 'off', 'i', \"don't\", \"needn't\", 'don', 'whom', 'these', 'isn', 'through', 've', 'very', 'it', 'm', \"that'll\", \"you'll\", 'mightn', 'him', 'about'}\n",
      "Number of stopwords: 198\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Stop Words :{stop_words}\")\n",
    "print(f\"Number of stopwords: {len(stop_words)}\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stopwords and lemmatize\n",
    "    processd_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(processd_words)\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(\"Preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893ea2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4712 stored elements and shape (9, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 369)\t0.8512306812056797\n",
      "  (0, 675)\t0.1792181330104813\n",
      "  (0, 190)\t0.04480453325262033\n",
      "  (0, 836)\t0.03627313109833374\n",
      "  (0, 857)\t0.022402266626310164\n",
      "  (0, 952)\t0.02510569981063823\n",
      "  (0, 869)\t0.020116546437431444\n",
      "  (0, 603)\t0.0444834922189141\n",
      "  (0, 399)\t0.04480453325262033\n",
      "  (0, 674)\t0.020116546437431444\n",
      "  (0, 620)\t0.03627313109833374\n",
      "  (0, 581)\t0.01813656554916687\n",
      "  (0, 505)\t0.016390098632453266\n",
      "  (0, 445)\t0.04023309287486289\n",
      "  (0, 546)\t0.014827830739638035\n",
      "  (0, 256)\t0.020116546437431444\n",
      "  (0, 243)\t0.014827830739638035\n",
      "  (0, 947)\t0.02965566147927607\n",
      "  (0, 512)\t0.020116546437431444\n",
      "  (0, 38)\t0.014827830739638035\n",
      "  (0, 983)\t0.016390098632453266\n",
      "  (0, 497)\t0.07531709943191468\n",
      "  (0, 822)\t0.022402266626310164\n",
      "  (0, 709)\t0.01813656554916687\n",
      "  (0, 490)\t0.01813656554916687\n",
      "  :\t:\n",
      "  (8, 167)\t0.007741752967795872\n",
      "  (8, 180)\t0.015483505935591745\n",
      "  (8, 521)\t0.023225258903387617\n",
      "  (8, 499)\t0.007741752967795872\n",
      "  (8, 938)\t0.007741752967795872\n",
      "  (8, 435)\t0.015483505935591745\n",
      "  (8, 185)\t0.007741752967795872\n",
      "  (8, 178)\t0.007741752967795872\n",
      "  (8, 627)\t0.03096701187118349\n",
      "  (8, 432)\t0.038708764838979365\n",
      "  (8, 563)\t0.023225258903387617\n",
      "  (8, 962)\t0.08515928264575459\n",
      "  (8, 390)\t0.007741752967795872\n",
      "  (8, 475)\t0.03096701187118349\n",
      "  (8, 677)\t0.007741752967795872\n",
      "  (8, 405)\t0.0541922707745711\n",
      "  (8, 162)\t0.046450517806775234\n",
      "  (8, 479)\t0.008903979294490594\n",
      "  (8, 870)\t0.008903979294490594\n",
      "  (8, 66)\t0.008903979294490594\n",
      "  (8, 238)\t0.062327855061434165\n",
      "  (8, 715)\t0.284927337423699\n",
      "  (8, 246)\t0.07123183435592476\n",
      "  (8, 413)\t0.062327855061434165\n",
      "  (8, 169)\t0.10542045299727737\n",
      "TF-IDF matrix created successfully\n",
      "Shape of the matrix: (9, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 : Covert Text to Vectors\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000) # Limit to top 1000 features\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(\"TF-IDF matrix created successfully\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c1aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [1 2 2 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "k =3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42,n_init=5)\n",
    "## the below line is fuctionally identical to the above line\n",
    "# kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"10\"\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get cluster labels for each document\n",
    "labels = kmeans.labels_\n",
    "print(f\"Cluster labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb17299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-Cluster Sum of Squares (WCSS): 4.869339711751118\n",
      "Silhouette Score: 0.08005379687201074\n"
     ]
    }
   ],
   "source": [
    "wcss = kmeans.inertia_\n",
    "\n",
    "sil_score = silhouette_score(tfidf_matrix, labels)\n",
    "\n",
    "print(f\"Within-Cluster Sum of Squares (WCSS): {wcss}\")\n",
    "print(f\"Silhouette Score: {sil_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
